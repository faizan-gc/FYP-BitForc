# -*- coding: utf-8 -*-
"""Important Functions

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12bCYfu73NIBeWlNOBLmD0vRkSihLxqPb

# Dependancies
"""

# Commented out IPython magic to ensure Python compatibility.
# !pip install pytrends
# !pip install statsmodels==0.12.1
# !pip install pmdarima
# !pip install aiohttp

# !wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz
# !tar -xzvf ta-lib-0.4.0-src.tar.gz
# # %cd ta-lib
# !./configure --prefix=/usr
# !make
# !make install
# !pip install Ta-Lib

import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np
from datetime import datetime, date, time, timedelta
import nest_asyncio
import json
import asyncio
from statsmodels.tsa.arima.model import ARIMA
import pmdarima as pm
import datetime
import pandas as pd
import datetime  
import talib
import numpy as np 
import pandas as pd 
from datetime import datetime
pd.options.mode.chained_assignment = None
import datetime as dt

import pandas as pd
import numpy as np
from datetime import datetime, date, time, timedelta
import nest_asyncio
nest_asyncio.apply()
import json
import asyncio
import aiohttp  
import datetime 
from datetime import datetime, timedelta, timezone

"""# Fetch OHLCV"""

polygon_endtime = datetime.now(timezone.utc).replace(microsecond=0, second=0, minute=0)
polygon_starttime = polygon_endtime - timedelta(days = 1, hours=0, minutes=60)
print(polygon_endtime)
print(polygon_starttime)


def daterange(date1, date2):
    for n in range(int((date2 - date1).days) + 1):
        yield date1 + timedelta(n)

data_dict = {}

async def get(
    session: aiohttp.ClientSession,
    date: str,
    **kwargs
) -> dict:
    global data_dict
    api = f"https://api.polygon.io/v2/aggs/ticker/X:BTCUSD/range/1/minute/{date}/{date}?adjusted=true&sort=asc&limit=1440&apiKey=Ot5XxPIdM4IAsPj6TdlIqHajQFK356JB"
    #print(f"Requesting {api}")
    resp = await session.request('GET', url=api, **kwargs)
    #print(resp)
    data = await resp.json()
    # print(data)
    data_dict[date] = data
    
async def main(dates, **kwargs):
    async with aiohttp.ClientSession() as session:
        tasks = []
        for c in dates:
            tasks.append(get(session=session, date=c, **kwargs))
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        return responses


if __name__ == '__main__':
    start_date = polygon_starttime
    end_date = polygon_endtime
    dates = []
    for i in daterange(pd.to_datetime(start_date), pd.to_datetime(end_date)):
        dates.append(i.date().strftime("%Y-%m-%d"))
    #print(dates)
    asyncio.run(main(dates))

def make_ohlcv():
  new_dict = []

  for index,i in enumerate(data_dict):
    if 'results' not in list(data_dict[i].keys()):
        pass
    else:
        new_dict = new_dict + data_dict[i]['results']

  df = pd.DataFrame(new_dict)
  df['timestamp'] = df['t']
  del df['t']
  df['timestamp'] = pd.to_datetime(df['timestamp'],unit = 'ms')
  df['timestamp'] = df['timestamp'].dt.tz_localize('UTC')
  #df['timestamp'] = df['timestamp'].dt.tz_convert('US/Eastern')
  df['timestamp'] = df['timestamp'].dt.tz_localize(None)
  df = df[['timestamp','o','h','l','c','v']]
  df = df.sort_values(by = 'timestamp')
  df = df.set_index("timestamp")
  df = df.resample('H').mean()
  return df

df = make_ohlcv()
df

"""# Fetch Tweets"""

!pip install vaderSentiment

!pip install swifter

from pandas.io.json import json_normalize

import tqdm
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import nltk
import swifter
nltk.download('stopwords')

def preprocess_json(path_):
    with open(path_, encoding="utf8") as f:
        d = json.load(f)
    dataset = json_normalize(d['data'])
    return dataset

def preprocess(dataset):
    
    dataset = dataset[["created_at", "text", "like_count", "retweet_count", "user_follower_count"]]
    dataset["created_at"] = pd.to_datetime(dataset["created_at"])
    dataset['created_at'] = dataset['created_at'].dt.tz_localize(None)

    dataset["text"] = dataset['text'].str.replace('http\S+|www.\S+', '', case=False)                                              #removing any links
    dataset["text"] = dataset["text"].str.replace('[^A-Za-z\s]+', '')                                                           #removing special chars
    stop = stopwords.words('english')
    dataset["text"] = dataset["text"].swifter.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))

    analyser = SentimentIntensityAnalyzer()
    scores = []
    for i in (range(len(dataset["text"]))):
      score = analyser.polarity_scores(dataset["text"][i])
      scores.append(score)
    dataset[["neg", "nuet", "pos", "compound"]] = pd.DataFrame(scores)
    dataset["like_count"] = pd.to_numeric(dataset.like_count, errors='coerce')
    dataset["retweet_count"] = pd.to_numeric(dataset.retweet_count, errors='coerce')
    if "user_follower_count" in dataset.columns:
        print("Follower count is available in this json!")
        dataset["user_follower_count"] = pd.to_numeric(dataset.user_follower_count, errors='coerce')
        dataset["FinalScore"] = dataset["compound"] * ((dataset["like_count"]) + 1) * ((dataset["retweet_count"]) + 1) * ((dataset["user_follower_count"]) + 1)
    else:
        print("Follower count is not available in this json.")
        dataset["FinalScore"] = dataset["compound"] * ((dataset["like_count"]) + 1) * ((dataset["retweet_count"]) + 1)

    dataset = dataset[["created_at", "text", "FinalScore"]]
    dataset = dataset.rename(columns={'created_at': 'timestamp'})

    return dataset

dff = preprocess_json("/content/recent.json")
new_df = preprocess(dff)
new_df

tweet_df = new_df[["timestamp", "FinalScore"]]
tweet_df["timestamp"] = pd.to_datetime(tweet_df["timestamp"])
tweet_df = tweet_df.sort_values(by = "timestamp")
tweet_df = tweet_df.reset_index(drop = True)
tweet_df = tweet_df.set_index("timestamp")
tweet_df = tweet_df.resample('60Min').mean()
last_score = tweet_df["FinalScore"].values[-1]
last_score

FinalScore = [0] * (len(df) -1)
FinalScore.append(last_score)
FinalScore

df["FinalScore"] = FinalScore
df = df.rename({"o" : "open", "h" : "high", "l" : "low", "c" : "close", "v" : "volume"}, axis = 1)
df

df.to_csv("/content/latest.csv")

"""# Data Prepare"""

def make_data_for_model(vader_df_path, ohlcv_path):
  # df2 = pd.read_csv(vader_df_path)
  # df2["timestamp"] = pd.to_datetime(df2["timestamp"])
  # df2["timestamp"] = df2["timestamp"].dt.tz_localize(None)
  # df2 = df2.resample('H').mean().ffill()

  # df = pd.read_csv(ohlcv_path)
  # df = df.set_index("timestamp")
  # df2 = df2.set_index("timestamp")
  # df_all = pd.merge(df,df2, how='inner', left_index=True, right_index=True)
  # df_all = df_all.dropna()
  # df = df_all.copy()

  df = pd.read_csv("/content/latest.csv")
  df["timestamp"] = pd.to_datetime(df["timestamp"])
  df = df.set_index("timestamp")

  df["open"] = df["open"].astype(float)
  df["high"] = df["high"].astype(float)
  df["low"] = df["low"].astype(float)
  df["volume"] = df["volume"].astype(float)
  df["close"] = df["close"].astype(float)
  #df = df.resample('H').mean().ffill()

  #df = df.dropna()

  df["close_pct_prev"] = df["close"].pct_change()
  df["EMA2"] = talib.EMA(df["close_pct_prev"], timeperiod=2)
  df["SMA2"] = talib.SMA(df["close_pct_prev"], timeperiod=2)
  df["RSI2"] = talib.RSI(df["close_pct_prev"], timeperiod=2)
  #df["ATR"] = talib.ATR(df["high"], df["low"], df["close"], timeperiod=14)


  df["open"] = df["open"].pct_change()
  df["high"] = df["high"].pct_change()
  df["low"] = df["low"].pct_change()
  df["FinalScore"] = df["FinalScore"]


  df["EMA2_open"] = talib.EMA(df["open"], timeperiod=2)
  df["EMA2_high"] = talib.EMA(df["high"], timeperiod=2)
  df["EMA2_low"] = talib.EMA(df["low"], timeperiod=2)

  df.replace([np.inf, -np.inf], np.nan, inplace=True)
  df = df.dropna()
  return df

df_fin = make_data_for_model("a", "b")
df_fin

df_fin.to_csv("/content/altuu.csv")

"""# Predict Function"""

import pickle

def predict_future(df_path, model_path):
  exg_features = ["EMA2_open", "EMA2_low", "EMA2", "SMA2", "FinalScore"]
  df = pd.read_csv(df_path)
  with open(model_path , 'rb') as f:
      model = pickle.load(f)
  forecast = model.predict(n_periods=len(df), exogenous = df[exg_features])
  df["Forecast_ARIMAX"] = forecast
  metrics_df = df[["timestamp", "Forecast_ARIMAX"]]
  return metrics_df

fin_df = predict_future("/content/altuu.csv", "/content/ARIMAX.pkl")
fin_df